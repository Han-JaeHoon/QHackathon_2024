{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from RNN_block import RNN_block\n",
    "import pennylane as qml\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7476e-02, 2.3959e+01, 3.5131e-02, 1.2807e+01])\n"
     ]
    }
   ],
   "source": [
    "n_train = 250\n",
    "\n",
    "y_train = torch.tensor(df['nat_demand'].to_numpy()[:n_train]).to(torch.float)\n",
    "x_train = torch.tensor(df[sorted(\"T2M_toc\tQV2M_toc TQL_toc W2M_toc T2M_san QV2M_san TQL_san W2M_san T2M_dav QV2M_dav TQL_dav W2M_dav\".split())].to_numpy()[:n_train]).to(torch.float)\n",
    "\n",
    "xs_train = torch.tensor([(x_train[:,3 * i : 3 * i + 3].sum(dim=1)/3).tolist() for i in range(4)]).T\n",
    "print(xs_train[1])\n",
    "y_train = (y_train-y_train.min())/(y_train.max()-y_train.min())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "class RNN_block:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "\n",
    "    \n",
    "    def embedding(self, params):\n",
    "        n = (self.input_size + 1) // 2\n",
    "        for i in range(n):\n",
    "            qml.Hadamard(i)\n",
    "            qml.RZ(2.0 * params[:,i], i)\n",
    "        \n",
    "        for i in range(n - 1):\n",
    "            qml.IsingZZ(2.0 * params[:,n + i] ,[i , i + 1])\n",
    "    \n",
    "    def ansatz(self, params, all_entangled = False):\n",
    "        # Length of Params : 3 * num_qubit\n",
    "        n = self.input_size\n",
    "        for i in range(n):\n",
    "            qml.RX(params[3 * i], i)\n",
    "            qml.RY(params[3 * i + 1], i)\n",
    "            qml.RZ(params[3 * i + 2], i)\n",
    "        for i in range(n - 1):\n",
    "            qml.CNOT([i, i + 1])\n",
    "        if all_entangled:\n",
    "            qml.CNOT([n - 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1(nn.Module):\n",
    "    '''\n",
    "    G for V(G)\n",
    "    '''\n",
    "    def __init__(self, input):\n",
    "        super(G1, self).__init__()\n",
    "        self.li1 = nn.Linear(input,16)\n",
    "        self.li2 = nn.Linear(16, 16)\n",
    "        self.li3 = nn.Linear(16, 2*input-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.li1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.li2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.li3(x)\n",
    "        x = 2*torch.pi*F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qu = 4\n",
    "dev = qml.device(\"default.qubit\", wires = n_qu)\n",
    "\n",
    "def embedding(params, n_qu):\n",
    "    '''\n",
    "    embedding layer\n",
    "    '''\n",
    "    n = n_qu\n",
    "    for i in range(n):\n",
    "        qml.Hadamard(i)\n",
    "        qml.RZ(2.0 * params[:,i], i)\n",
    "    \n",
    "    for i in range(n - 1):\n",
    "        qml.IsingZZ(2.0 * params[:,n + i] ,[i , i + 1])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def fidelity(vec1, vec2, n_qu):\n",
    "    '''\n",
    "        Args:\n",
    "            vec1 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "            vec2 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "    '''\n",
    "    embedding(vec1, n_qu) # Phi(x1) circuit 적용\n",
    "    qml.adjoint(embedding)(vec2, n_qu) # Phi^t(x2) 적용\n",
    "    return qml.probs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN\n",
    "from fucntions import data_seq,train_seq\n",
    "class feature_train(nn.Module):\n",
    "    def __init__(self,n_qu):\n",
    "        super(feature_train, self).__init__()\n",
    "        self.n_qu = n_qu\n",
    "        self.linear1 = KAN([n_qu,n_qu*2+1,n_qu*2-1],grid=1)\n",
    "        self.quantum_layer = fidelity\n",
    "    def forward(self,inputs):\n",
    "        input1 = inputs[0]\n",
    "        input2 = inputs[1]\n",
    "        input1 = self.linear1(input1)\n",
    "        input2 = self.linear1(input2)\n",
    "        output = self.quantum_layer(input1,input2,self.n_qu)[:,0]\n",
    "        return output\n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "train_label_list = []\n",
    "test_list = []\n",
    "test_label_list = []\n",
    "\n",
    "for i in range(n_train-1):\n",
    "    train_data = torch.stack([xs_train,torch.concat([xs_train[(i+1):],xs_train[:(i+1)]])])\n",
    "    train_list.append(train_data)\n",
    "    label_data = torch.stack([y_train,torch.concat([y_train[(i+1):],y_train[:(i+1)]])])\n",
    "    train_label_list.append(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.concat(train_list,dim=1)\n",
    "train_label = torch.concat(train_label_list,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "pretrain_data = data_seq(train_data,train_label)\n",
    "train_loader, test_loader = pretrain_data.split_data(batch_size=64,seq_first=True)\n",
    "model = feature_train(n_qu)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
    "\n",
    "def criterion(pred,label):\n",
    "    return -torch.sum(torch.log(1-pred)*(label[:,0]-label[:,1])**2 + torch.log(pred)*(1-(label[:,0]-label[:,1])**2))/len(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[507], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pretrain_seq \u001b[38;5;241m=\u001b[39m train_seq(model,train_loader,test_loader)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpretrain_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[503], line 78\u001b[0m, in \u001b[0;36mtrain_seq.train\u001b[1;34m(self, epochs, optimizer, criterion, seq_first)\u001b[0m\n\u001b[0;32m     76\u001b[0m     label_list\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred,label)\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     80\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(pred_list)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_seq = train_seq(model,train_loader,test_loader)\n",
    "pretrain_seq.train(3,optimizer,criterion,seq_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = RNN_block(4)\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def ansatz(inputs,weights):\n",
    "    '''\n",
    "        Args:\n",
    "            vec1 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "            vec2 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "    '''\n",
    "    block.embedding(inputs)\n",
    "    block.ansatz(weights) # Phi(x1) circuit 적용\n",
    "    return qml.probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = RNN_block(4)\n",
    "class train_model(nn.Module):\n",
    "    def __init__(self,feature_model):\n",
    "        super(train_model, self).__init__()\n",
    "        self.QNE_layer = feature_model.linear1\n",
    "        self.ansatz = ansatz\n",
    "        self.Q_param = nn.Parameter(torch.rand([12]).float(),requires_grad=True)\n",
    "    def forward(self,input):\n",
    "        input = self.QNE_layer(input)\n",
    "        output = self.ansatz(input,self.Q_param)[:,0]\n",
    "        return output\n",
    "    \n",
    "class train_model_fixed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(train_model_fixed, self).__init__()\n",
    "        self.ansatz = ansatz\n",
    "        self.Q_param = nn.Parameter(torch.rand([12]).float(),requires_grad=True)\n",
    "    def forward(self,input):\n",
    "        data_list = []\n",
    "        feature_len = input.shape[1]\n",
    "        for i in range(feature_len-1):\n",
    "            data = (np.pi-input[:,i])*(np.pi-input[:,i+1])\n",
    "            data_list.append(data)\n",
    "        data = torch.stack(data_list,dim=1)\n",
    "        input = torch.concat([input,data],dim=1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        output = self.ansatz(input,self.Q_param)[:,0]\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_seq(xs_train,y_train)\n",
    "train_loader,test_loader = data.split_data(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_model  = train_model(model)\n",
    "optimizer = optim.Adam([CLS_model.Q_param],lr=0.04)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "reg_seq = train_seq(CLS_model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 loss :0.18464331328868866 loss_test = 0.20358189940452576\n",
      "epoch : 2 loss :0.153375506401062 loss_test = 0.1592804491519928\n",
      "epoch : 3 loss :0.11685360223054886 loss_test = 0.11393798887729645\n",
      "epoch : 4 loss :0.08523280918598175 loss_test = 0.08010075241327286\n",
      "epoch : 5 loss :0.06789699196815491 loss_test = 0.06528328359127045\n",
      "epoch : 6 loss :0.06679517775774002 loss_test = 0.06347034871578217\n",
      "epoch : 7 loss :0.07087789475917816 loss_test = 0.06357274204492569\n",
      "epoch : 8 loss :0.06964418292045593 loss_test = 0.06408829987049103\n",
      "epoch : 9 loss :0.06549665331840515 loss_test = 0.06908824294805527\n",
      "epoch : 10 loss :0.06548474729061127 loss_test = 0.07465886324644089\n",
      "epoch : 11 loss :0.06769930571317673 loss_test = 0.07623013854026794\n",
      "epoch : 12 loss :0.06742370128631592 loss_test = 0.0711924359202385\n",
      "epoch : 13 loss :0.0655263364315033 loss_test = 0.06798989325761795\n",
      "epoch : 14 loss :0.06494953483343124 loss_test = 0.06538277119398117\n",
      "epoch : 15 loss :0.06551557779312134 loss_test = 0.06472069025039673\n",
      "epoch : 16 loss :0.06575319916009903 loss_test = 0.06516741961240768\n",
      "epoch : 17 loss :0.06575264781713486 loss_test = 0.06708011031150818\n",
      "epoch : 18 loss :0.06510874629020691 loss_test = 0.06898771226406097\n",
      "epoch : 19 loss :0.06549707800149918 loss_test = 0.07194536924362183\n",
      "epoch : 20 loss :0.06621361523866653 loss_test = 0.07112526148557663\n",
      "epoch : 21 loss :0.0656922310590744 loss_test = 0.06828559190034866\n",
      "epoch : 22 loss :0.0652032122015953 loss_test = 0.06694414466619492\n",
      "epoch : 23 loss :0.0651385709643364 loss_test = 0.066655732691288\n",
      "epoch : 24 loss :0.0652426928281784 loss_test = 0.06662772595882416\n",
      "epoch : 25 loss :0.06536240875720978 loss_test = 0.06732779741287231\n",
      "epoch : 26 loss :0.06510917842388153 loss_test = 0.06638625264167786\n",
      "epoch : 27 loss :0.06516752392053604 loss_test = 0.06587409973144531\n",
      "epoch : 28 loss :0.06537676602602005 loss_test = 0.06552678346633911\n",
      "epoch : 29 loss :0.06540166586637497 loss_test = 0.06635850667953491\n",
      "epoch : 30 loss :0.06513836234807968 loss_test = 0.0695386677980423\n",
      "epoch : 31 loss :0.06629643589258194 loss_test = 0.07232152670621872\n",
      "epoch : 32 loss :0.06591708958148956 loss_test = 0.0683605745434761\n",
      "epoch : 33 loss :0.06485805660486221 loss_test = 0.06461258977651596\n",
      "epoch : 34 loss :0.06655013561248779 loss_test = 0.06363731622695923\n",
      "epoch : 35 loss :0.06682430952787399 loss_test = 0.06491319835186005\n",
      "epoch : 36 loss :0.06495390087366104 loss_test = 0.06883163750171661\n",
      "epoch : 37 loss :0.06520503014326096 loss_test = 0.07111421227455139\n",
      "epoch : 38 loss :0.06606678664684296 loss_test = 0.0719510018825531\n",
      "epoch : 39 loss :0.06605382263660431 loss_test = 0.06977872550487518\n",
      "epoch : 40 loss :0.06545198708772659 loss_test = 0.06590227782726288\n",
      "epoch : 41 loss :0.06537722796201706 loss_test = 0.06523896008729935\n",
      "epoch : 42 loss :0.06560911983251572 loss_test = 0.06475713849067688\n",
      "epoch : 43 loss :0.06575387716293335 loss_test = 0.06474724411964417\n",
      "epoch : 44 loss :0.0657537654042244 loss_test = 0.06485491245985031\n",
      "epoch : 45 loss :0.06560671329498291 loss_test = 0.06591537594795227\n",
      "epoch : 46 loss :0.06505236029624939 loss_test = 0.06896567344665527\n",
      "epoch : 47 loss :0.06570710241794586 loss_test = 0.0721392035484314\n",
      "epoch : 48 loss :0.06597016006708145 loss_test = 0.0694955512881279\n",
      "epoch : 49 loss :0.06517226248979568 loss_test = 0.06691092997789383\n",
      "epoch : 50 loss :0.06517855077981949 loss_test = 0.06678522378206253\n",
      "epoch : 51 loss :0.06509643793106079 loss_test = 0.06576067209243774\n",
      "epoch : 52 loss :0.06534732133150101 loss_test = 0.06539663672447205\n",
      "epoch : 53 loss :0.06540733575820923 loss_test = 0.06688757240772247\n",
      "epoch : 54 loss :0.06507359445095062 loss_test = 0.06886722147464752\n",
      "epoch : 55 loss :0.06556890159845352 loss_test = 0.07208843529224396\n",
      "epoch : 56 loss :0.06643669307231903 loss_test = 0.07428981363773346\n",
      "epoch : 57 loss :0.06703261286020279 loss_test = 0.07396314293146133\n",
      "epoch : 58 loss :0.06676042824983597 loss_test = 0.072410948574543\n",
      "epoch : 59 loss :0.06619413197040558 loss_test = 0.07075342535972595\n",
      "epoch : 60 loss :0.06551077216863632 loss_test = 0.0674591064453125\n",
      "epoch : 61 loss :0.06515108048915863 loss_test = 0.06530287116765976\n",
      "epoch : 62 loss :0.06574489921331406 loss_test = 0.06410469859838486\n",
      "epoch : 63 loss :0.0663551390171051 loss_test = 0.06438858062028885\n",
      "epoch : 64 loss :0.06593306362628937 loss_test = 0.06549034267663956\n",
      "epoch : 65 loss :0.06533036381006241 loss_test = 0.06743946671485901\n",
      "epoch : 66 loss :0.06525655090808868 loss_test = 0.0698121041059494\n",
      "epoch : 67 loss :0.065553218126297 loss_test = 0.07188132405281067\n",
      "epoch : 68 loss :0.06611376255750656 loss_test = 0.07135326415300369\n",
      "epoch : 69 loss :0.06577162444591522 loss_test = 0.06919002532958984\n",
      "epoch : 70 loss :0.06529267132282257 loss_test = 0.0673915445804596\n",
      "epoch : 71 loss :0.06522306799888611 loss_test = 0.06691362708806992\n",
      "epoch : 72 loss :0.0651579350233078 loss_test = 0.06804201006889343\n",
      "epoch : 73 loss :0.06521911919116974 loss_test = 0.0689844861626625\n",
      "epoch : 74 loss :0.06541061401367188 loss_test = 0.0681200698018074\n",
      "epoch : 75 loss :0.06524819880723953 loss_test = 0.06854594498872757\n",
      "epoch : 76 loss :0.06533097475767136 loss_test = 0.07027089595794678\n",
      "epoch : 77 loss :0.0655679777264595 loss_test = 0.06877343356609344\n",
      "epoch : 78 loss :0.0652695819735527 loss_test = 0.0682184100151062\n",
      "epoch : 79 loss :0.06521732360124588 loss_test = 0.06695794314146042\n",
      "epoch : 80 loss :0.06533033400774002 loss_test = 0.06468810886144638\n",
      "epoch : 81 loss :0.0659896582365036 loss_test = 0.06427541375160217\n",
      "epoch : 82 loss :0.0662749782204628 loss_test = 0.06444054841995239\n",
      "epoch : 83 loss :0.06585568934679031 loss_test = 0.06596794724464417\n",
      "epoch : 84 loss :0.06569192558526993 loss_test = 0.06908655166625977\n",
      "epoch : 85 loss :0.06530769169330597 loss_test = 0.06844951957464218\n",
      "epoch : 86 loss :0.06532017886638641 loss_test = 0.06809288263320923\n",
      "epoch : 87 loss :0.06538143754005432 loss_test = 0.06975844502449036\n",
      "epoch : 88 loss :0.0656680166721344 loss_test = 0.07162575423717499\n",
      "epoch : 89 loss :0.06579113751649857 loss_test = 0.06852970272302628\n",
      "epoch : 90 loss :0.06499132513999939 loss_test = 0.06608670204877853\n",
      "epoch : 91 loss :0.06511090695858002 loss_test = 0.06420578807592392\n",
      "epoch : 92 loss :0.06661055237054825 loss_test = 0.06377876549959183\n",
      "epoch : 93 loss :0.06678469479084015 loss_test = 0.06450416892766953\n",
      "epoch : 94 loss :0.06556504219770432 loss_test = 0.06690194457769394\n",
      "epoch : 95 loss :0.06522855907678604 loss_test = 0.07169411331415176\n",
      "epoch : 96 loss :0.06657107174396515 loss_test = 0.07552476227283478\n",
      "epoch : 97 loss :0.0676039606332779 loss_test = 0.07447302341461182\n",
      "epoch : 98 loss :0.06671370565891266 loss_test = 0.07061219960451126\n",
      "epoch : 99 loss :0.06562639027833939 loss_test = 0.06794930249452591\n",
      "epoch : 100 loss :0.06513627618551254 loss_test = 0.06710594892501831\n"
     ]
    }
   ],
   "source": [
    "reg_seq.train(100,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_model  = train_model_fixed()\n",
    "optimizer = optim.Adam([CLS_model.Q_param],lr=0.04)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "reg_seq = train_seq(CLS_model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 loss :0.22131291031837463 loss_test = 0.2660052180290222\n",
      "epoch : 2 loss :0.2125205099582672 loss_test = 0.252135694026947\n",
      "epoch : 3 loss :0.19893859326839447 loss_test = 0.23267321288585663\n",
      "epoch : 4 loss :0.18025341629981995 loss_test = 0.20858950912952423\n",
      "epoch : 5 loss :0.15985174477100372 loss_test = 0.18297429382801056\n",
      "epoch : 6 loss :0.13945746421813965 loss_test = 0.15924471616744995\n",
      "epoch : 7 loss :0.1214638501405716 loss_test = 0.13988447189331055\n",
      "epoch : 8 loss :0.10846792906522751 loss_test = 0.12543658912181854\n",
      "epoch : 9 loss :0.09766899049282074 loss_test = 0.1136385127902031\n",
      "epoch : 10 loss :0.0886048972606659 loss_test = 0.10089411586523056\n",
      "epoch : 11 loss :0.07941437512636185 loss_test = 0.08706020563840866\n",
      "epoch : 12 loss :0.06996604055166245 loss_test = 0.07336292415857315\n",
      "epoch : 13 loss :0.06227102875709534 loss_test = 0.06241871044039726\n",
      "epoch : 14 loss :0.057357307523489 loss_test = 0.055275898426771164\n",
      "epoch : 15 loss :0.05466345697641373 loss_test = 0.0501171238720417\n",
      "epoch : 16 loss :0.053346987813711166 loss_test = 0.0481557734310627\n",
      "epoch : 17 loss :0.05459672957658768 loss_test = 0.048395924270153046\n",
      "epoch : 18 loss :0.055071864277124405 loss_test = 0.04789113625884056\n",
      "epoch : 19 loss :0.054467130452394485 loss_test = 0.04762258380651474\n",
      "epoch : 20 loss :0.053804825991392136 loss_test = 0.04819488152861595\n",
      "epoch : 21 loss :0.05343450605869293 loss_test = 0.04989658296108246\n",
      "epoch : 22 loss :0.05417163670063019 loss_test = 0.050773877650499344\n",
      "epoch : 23 loss :0.05454743281006813 loss_test = 0.05144265294075012\n",
      "epoch : 24 loss :0.05420392006635666 loss_test = 0.05199631303548813\n",
      "epoch : 25 loss :0.054041340947151184 loss_test = 0.05172334611415863\n",
      "epoch : 26 loss :0.053530145436525345 loss_test = 0.050152406096458435\n",
      "epoch : 27 loss :0.05360344797372818 loss_test = 0.04791844263672829\n",
      "epoch : 28 loss :0.05336318537592888 loss_test = 0.04723938927054405\n",
      "epoch : 29 loss :0.05365591496229172 loss_test = 0.04750408977270126\n",
      "epoch : 30 loss :0.053805164992809296 loss_test = 0.04784051328897476\n",
      "epoch : 31 loss :0.053687747567892075 loss_test = 0.04868270829319954\n",
      "epoch : 32 loss :0.053358662873506546 loss_test = 0.0496528260409832\n",
      "epoch : 33 loss :0.054066408425569534 loss_test = 0.050995245575904846\n",
      "epoch : 34 loss :0.05390080064535141 loss_test = 0.051486626267433167\n",
      "epoch : 35 loss :0.05366602540016174 loss_test = 0.050967928022146225\n",
      "epoch : 36 loss :0.053808778524398804 loss_test = 0.050006236881017685\n",
      "epoch : 37 loss :0.05365804582834244 loss_test = 0.04951911419630051\n",
      "epoch : 38 loss :0.053829144686460495 loss_test = 0.0494299978017807\n",
      "epoch : 39 loss :0.0538150779902935 loss_test = 0.049200739711523056\n",
      "epoch : 40 loss :0.053727272897958755 loss_test = 0.04898708686232567\n",
      "epoch : 41 loss :0.053541019558906555 loss_test = 0.04860549792647362\n",
      "epoch : 42 loss :0.05377346649765968 loss_test = 0.04921911284327507\n",
      "epoch : 43 loss :0.05390023812651634 loss_test = 0.049139704555273056\n",
      "epoch : 44 loss :0.054111529141664505 loss_test = 0.04955057054758072\n",
      "epoch : 45 loss :0.05412675812840462 loss_test = 0.04883769899606705\n",
      "epoch : 46 loss :0.05373549461364746 loss_test = 0.048330117017030716\n",
      "epoch : 47 loss :0.0533599816262722 loss_test = 0.048244282603263855\n",
      "epoch : 48 loss :0.053365979343652725 loss_test = 0.04854866489768028\n",
      "epoch : 49 loss :0.0535166934132576 loss_test = 0.04888291656970978\n",
      "epoch : 50 loss :0.053558897227048874 loss_test = 0.049240466207265854\n",
      "epoch : 51 loss :0.053686704486608505 loss_test = 0.04982256516814232\n",
      "epoch : 52 loss :0.05372711643576622 loss_test = 0.05019921436905861\n",
      "epoch : 53 loss :0.053561754524707794 loss_test = 0.05035460367798805\n",
      "epoch : 54 loss :0.053749337792396545 loss_test = 0.05109870806336403\n",
      "epoch : 55 loss :0.05392008274793625 loss_test = 0.05074746906757355\n",
      "epoch : 56 loss :0.05377088487148285 loss_test = 0.05028105154633522\n",
      "epoch : 57 loss :0.05325211584568024 loss_test = 0.050234612077474594\n",
      "epoch : 58 loss :0.05381371080875397 loss_test = 0.050199076533317566\n",
      "epoch : 59 loss :0.054084133356809616 loss_test = 0.04968623071908951\n",
      "epoch : 60 loss :0.054162997752428055 loss_test = 0.04908445477485657\n",
      "epoch : 61 loss :0.053661756217479706 loss_test = 0.04822273552417755\n",
      "epoch : 62 loss :0.05355020612478256 loss_test = 0.04814286157488823\n",
      "epoch : 63 loss :0.05373476445674896 loss_test = 0.04851899296045303\n",
      "epoch : 64 loss :0.053421422839164734 loss_test = 0.04881255701184273\n",
      "epoch : 65 loss :0.053343672305345535 loss_test = 0.04846816137433052\n",
      "epoch : 66 loss :0.05321735516190529 loss_test = 0.04788801074028015\n",
      "epoch : 67 loss :0.0539131835103035 loss_test = 0.04777352511882782\n",
      "epoch : 68 loss :0.05436699837446213 loss_test = 0.04802360013127327\n",
      "epoch : 69 loss :0.05447930842638016 loss_test = 0.04831595718860626\n",
      "epoch : 70 loss :0.05423867329955101 loss_test = 0.048800982534885406\n",
      "epoch : 71 loss :0.0536801852285862 loss_test = 0.049206189811229706\n",
      "epoch : 72 loss :0.05357222259044647 loss_test = 0.05033339932560921\n",
      "epoch : 73 loss :0.053767457604408264 loss_test = 0.05168524384498596\n",
      "epoch : 74 loss :0.05418257787823677 loss_test = 0.05176191404461861\n",
      "epoch : 75 loss :0.053476523607969284 loss_test = 0.05032090097665787\n",
      "epoch : 76 loss :0.05351942032575607 loss_test = 0.050156369805336\n",
      "epoch : 77 loss :0.053901150822639465 loss_test = 0.05017389357089996\n",
      "epoch : 78 loss :0.05422108992934227 loss_test = 0.04991476237773895\n",
      "epoch : 79 loss :0.0535908117890358 loss_test = 0.048947662115097046\n",
      "epoch : 80 loss :0.05351744592189789 loss_test = 0.04918193817138672\n",
      "epoch : 81 loss :0.05362728610634804 loss_test = 0.0491289347410202\n",
      "epoch : 82 loss :0.053357213735580444 loss_test = 0.04907674342393875\n",
      "epoch : 83 loss :0.05329417809844017 loss_test = 0.04921923950314522\n",
      "epoch : 84 loss :0.05332876369357109 loss_test = 0.049181029200553894\n",
      "epoch : 85 loss :0.05333942547440529 loss_test = 0.048925742506980896\n",
      "epoch : 86 loss :0.05348559468984604 loss_test = 0.04871958866715431\n",
      "epoch : 87 loss :0.053484365344047546 loss_test = 0.049011774361133575\n",
      "epoch : 88 loss :0.05331422761082649 loss_test = 0.04900633916258812\n",
      "epoch : 89 loss :0.053306903690099716 loss_test = 0.04892876744270325\n",
      "epoch : 90 loss :0.053403422236442566 loss_test = 0.04866619408130646\n",
      "epoch : 91 loss :0.053390707820653915 loss_test = 0.04947897419333458\n",
      "epoch : 92 loss :0.05364416539669037 loss_test = 0.05110349506139755\n",
      "epoch : 93 loss :0.05387456342577934 loss_test = 0.0516146756708622\n",
      "epoch : 94 loss :0.05374553054571152 loss_test = 0.0511871762573719\n",
      "epoch : 95 loss :0.05354754626750946 loss_test = 0.050781622529029846\n",
      "epoch : 96 loss :0.053570643067359924 loss_test = 0.04976784810423851\n",
      "epoch : 97 loss :0.05364157631993294 loss_test = 0.04900328814983368\n",
      "epoch : 98 loss :0.0536971278488636 loss_test = 0.04862102121114731\n",
      "epoch : 99 loss :0.054044581949710846 loss_test = 0.04916490614414215\n",
      "epoch : 100 loss :0.05414464324712753 loss_test = 0.04998951032757759\n"
     ]
    }
   ],
   "source": [
    "reg_seq.train(100,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penny_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
