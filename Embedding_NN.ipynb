{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from RNN_block import RNN_block\n",
    "import pennylane as qml\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7476e-02, 2.3959e+01, 3.5131e-02, 1.2807e+01])\n"
     ]
    }
   ],
   "source": [
    "n_train = 250\n",
    "\n",
    "y_train = torch.tensor(df['nat_demand'].to_numpy()[:n_train]).to(torch.float)\n",
    "x_train = torch.tensor(df[sorted(\"T2M_toc\tQV2M_toc TQL_toc W2M_toc T2M_san QV2M_san TQL_san W2M_san T2M_dav QV2M_dav TQL_dav W2M_dav\".split())].to_numpy()[:n_train]).to(torch.float)\n",
    "\n",
    "xs_train = torch.tensor([(x_train[:,3 * i : 3 * i + 3].sum(dim=1)/3).tolist() for i in range(4)]).T\n",
    "print(xs_train[1])\n",
    "y_train = (y_train-y_train.min())/(y_train.max()-y_train.min())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "class RNN_block:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "\n",
    "    \n",
    "    def embedding(self, params):\n",
    "        n = (self.input_size + 1) // 2\n",
    "        for i in range(n):\n",
    "            qml.Hadamard(i)\n",
    "            qml.RZ(2.0 * params[:,i], i)\n",
    "        \n",
    "        for i in range(n - 1):\n",
    "            qml.IsingZZ(2.0 * params[:,n + i] ,[i , i + 1])\n",
    "    \n",
    "    def ansatz(self, params, all_entangled = False):\n",
    "        # Length of Params : 3 * num_qubit\n",
    "        n = self.input_size\n",
    "        for i in range(n):\n",
    "            qml.RX(params[3 * i], i)\n",
    "            qml.RY(params[3 * i + 1], i)\n",
    "            qml.RZ(params[3 * i + 2], i)\n",
    "        for i in range(n - 1):\n",
    "            qml.CNOT([i, i + 1])\n",
    "        if all_entangled:\n",
    "            qml.CNOT([n - 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1(nn.Module):\n",
    "    '''\n",
    "    G for V(G)\n",
    "    '''\n",
    "    def __init__(self, input):\n",
    "        super(G1, self).__init__()\n",
    "        self.li1 = nn.Linear(input,16)\n",
    "        self.li2 = nn.Linear(16, 16)\n",
    "        self.li3 = nn.Linear(16, 2*input-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.li1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.li2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.li3(x)\n",
    "        x = 2*torch.pi*F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qu = 4\n",
    "dev = qml.device(\"default.qubit\", wires = n_qu)\n",
    "\n",
    "def embedding(params, n_qu):\n",
    "    '''\n",
    "    embedding layer\n",
    "    '''\n",
    "    n = n_qu\n",
    "    for i in range(n):\n",
    "        qml.Hadamard(i)\n",
    "        qml.RZ(2.0 * params[:,i], i)\n",
    "    \n",
    "    for i in range(n - 1):\n",
    "        qml.IsingZZ(2.0 * params[:,n + i] ,[i , i + 1])\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def fidelity(vec1, vec2, n_qu):\n",
    "    '''\n",
    "        Args:\n",
    "            vec1 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "            vec2 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "    '''\n",
    "    embedding(vec1, n_qu) # Phi(x1) circuit 적용\n",
    "    qml.adjoint(embedding)(vec2, n_qu) # Phi^t(x2) 적용\n",
    "    return qml.probs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_train(nn.Module):\n",
    "    def __init__(self,n_qu):\n",
    "        super(feature_train, self).__init__()\n",
    "        self.n_qu = n_qu\n",
    "        self.linear1 = G1(input=n_qu)\n",
    "        self.quantum_layer = fidelity\n",
    "    def forward(self,inputs):\n",
    "        input1 = inputs[0]\n",
    "        input2 = inputs[1]\n",
    "        input1 = self.linear1(input1)\n",
    "        input2 = self.linear1(input2)\n",
    "        output = self.quantum_layer(input1,input2,self.n_qu)[:,0]\n",
    "        return output\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split     \n",
    "\n",
    "class data_seq():\n",
    "    def __init__(self,feature_data,label_data):\n",
    "        self.feature = feature_data\n",
    "        self.label = label_data\n",
    "    \n",
    "    def split_data(self,test_ratio=0.2, batch_size=32,seq_first = False):\n",
    "        \"\"\"\n",
    "        데이터를 훈련 및 테스트 세트로 나누고 배치 단위로 나누어 줍니다.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): 입력 특징 텐서\n",
    "            labels (torch.Tensor): 레이블 텐서\n",
    "            test_ratio (float): 테스트 세트 비율 (기본값: 0.2)\n",
    "            batch_size (int): 배치 크기 (기본값: 32)\n",
    "\n",
    "        Returns:\n",
    "            train_loader (DataLoader): 훈련 데이터 로더\n",
    "            test_loader (DataLoader): 테스트 데이터 로더\n",
    "        \"\"\"\n",
    "        if seq_first:\n",
    "            self.feature = self.feature.permute(1,0,2)\n",
    "            self.label = self.label.permute(1,0)\n",
    "        \n",
    "        # 데이터셋 생성\n",
    "        dataset = TensorDataset(self.feature, self.label)\n",
    "\n",
    "        # 훈련 및 테스트 데이터셋 크기 계산\n",
    "        test_size = int(len(dataset) * test_ratio)\n",
    "        train_size = len(dataset) - test_size\n",
    "\n",
    "        # 데이터셋 분할\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        # 데이터 로더 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "class train_seq():\n",
    "    def __init__(self,model,train_loader,test_loader):\n",
    "        self.model= model\n",
    "        self.train_data = train_loader\n",
    "        self.test_data = test_loader\n",
    "    def train(self,epochs,optimizer,criterion,seq_first = False):\n",
    "        for epoch in range(epochs):\n",
    "            pred_list = []\n",
    "            label_list = []\n",
    "            for train,label in self.train_data:\n",
    "                if seq_first:\n",
    "                    train = train.permute(1,0,2)\n",
    "                    label = label.permute(0,1)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(train)\n",
    "                pred_list.append(pred)\n",
    "                label_list.append(label)\n",
    "                loss = criterion(pred,label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            pred = torch.concat(pred_list)\n",
    "            label = torch.concat(label_list)\n",
    "            loss = criterion(pred,label)\n",
    "            loss_test = self.test(criterion,seq_first)\n",
    "            print(f'epoch : {epoch+1} loss :{loss} loss_test = {loss_test}')\n",
    "    def test(self,criterion,seq_first = False):\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        for test,label in self.test_data:\n",
    "            if seq_first:\n",
    "                test = test.permute(1,0,2)\n",
    "                label = label.permute(0,1)\n",
    "            pred = self.model(test)\n",
    "            pred_list.append(pred)\n",
    "            label_list.append(label)\n",
    "        pred = torch.concat(pred_list)\n",
    "        label = torch.concat(label_list)\n",
    "        loss = criterion(pred,label)\n",
    "        return loss\n",
    "        \n",
    "                \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "train_label_list = []\n",
    "test_list = []\n",
    "test_label_list = []\n",
    "\n",
    "for i in range(n_train-1):\n",
    "    train_data = torch.stack([xs_train,torch.concat([xs_train[(i+1):],xs_train[:(i+1)]])])\n",
    "    train_list.append(train_data)\n",
    "    label_data = torch.stack([y_train,torch.concat([y_train[(i+1):],y_train[:(i+1)]])])\n",
    "    train_label_list.append(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.concat(train_list,dim=1)\n",
    "train_label = torch.concat(train_label_list,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "pretrain_data = data_seq(train_data,train_label)\n",
    "train_loader, test_loader = pretrain_data.split_data(batch_size=64,seq_first=True)\n",
    "model = feature_train(n_qu)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.005)\n",
    "\n",
    "def criterion(pred,label):\n",
    "    return -torch.sum(torch.log(1-pred)*(label[:,0]-label[:,1])**2 + torch.log(pred)*(1-(label[:,0]-label[:,1])**2))/len(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 loss :1.5986825227737427 loss_test = 1.4027186632156372\n",
      "epoch : 1 loss :1.3776195049285889 loss_test = 1.3845701217651367\n",
      "epoch : 2 loss :1.3815345764160156 loss_test = 1.3753087520599365\n"
     ]
    }
   ],
   "source": [
    "pretrain_seq = train_seq(model,train_loader,test_loader)\n",
    "pretrain_seq.train(3,optimizer,criterion,seq_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = RNN_block(4)\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def ansatz(inputs,weights):\n",
    "    '''\n",
    "        Args:\n",
    "            vec1 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "            vec2 : list, (2n - 1)개의 element로 이루어진 vector\n",
    "    '''\n",
    "    block.embedding(inputs)\n",
    "    block.ansatz(weights) # Phi(x1) circuit 적용\n",
    "    return qml.probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = RNN_block(4)\n",
    "class train_model(nn.Module):\n",
    "    def __init__(self,feature_model):\n",
    "        super(train_model, self).__init__()\n",
    "        self.QNE_layer = feature_model.linear1\n",
    "        self.ansatz = ansatz\n",
    "        self.Q_param = nn.Parameter(torch.rand([12]).float(),requires_grad=True)\n",
    "    def forward(self,input):\n",
    "        input = self.QNE_layer(input)\n",
    "        output = self.ansatz(input,self.Q_param)[:,3]\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_seq(xs_train,y_train)\n",
    "train_loader,test_loader = data.split_data(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_model  = train_model(model)\n",
    "optimizer = optim.Adam([CLS_model.Q_param],lr=0.04)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "reg_seq = train_seq(model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[418], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreg_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[399], line 72\u001b[0m, in \u001b[0;36mtrain_seq.train\u001b[1;34m(self, epochs, optimizer, criterion, seq_first)\u001b[0m\n\u001b[0;32m     69\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 72\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m pred_list\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     74\u001b[0m label_list\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[399], line 12\u001b[0m, in \u001b[0;36mfeature_train.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     10\u001b[0m input1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(input1)\n\u001b[0;32m     11\u001b[0m input2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(input2)\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantum_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_qu\u001b[49m\u001b[43m)\u001b[49m[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:1092\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     override_shots \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshots\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m original_grad_fn \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice]\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn(shots\u001b[38;5;241m=\u001b[39moverride_shots, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:929\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_interface(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mqueuing\u001b[38;5;241m.\u001b[39mAnnotatedQueue() \u001b[38;5;28;01mas\u001b[39;00m q:\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape \u001b[38;5;241m=\u001b[39m QuantumScript\u001b[38;5;241m.\u001b[39mfrom_queue(q, shots)\n\u001b[0;32m    933\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[326], line 23\u001b[0m, in \u001b[0;36mfidelity\u001b[1;34m(vec1, vec2, n_qu)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@qml\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(dev, interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfidelity\u001b[39m(vec1, vec2, n_qu):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m        Args:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m            vec1 : list, (2n - 1)개의 element로 이루어진 vector\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m            vec2 : list, (2n - 1)개의 element로 이루어진 vector\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_qu\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Phi(x1) circuit 적용\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     qml\u001b[38;5;241m.\u001b[39madjoint(embedding)(vec2, n_qu) \u001b[38;5;66;03m# Phi^t(x2) 적용\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mprobs()\n",
      "Cell \u001b[1;32mIn[326], line 11\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(params, n_qu)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     10\u001b[0m     qml\u001b[38;5;241m.\u001b[39mHadamard(i)\n\u001b[1;32m---> 11\u001b[0m     qml\u001b[38;5;241m.\u001b[39mRZ(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, i)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     14\u001b[0m     qml\u001b[38;5;241m.\u001b[39mIsingZZ(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m params[:,n \u001b[38;5;241m+\u001b[39m i] ,[i , i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "reg_seq.train(5,optimizer,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penny_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
