{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from torch import nn\n",
    "from kan import KAN\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from RNN_block import RNN_block\n",
    "\n",
    "\n",
    "n_qubits = 8\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs1,inputs2, weights1,weights2):\n",
    "    block = RNN_block(8)\n",
    "    block.embedding(inputs1)\n",
    "    block.ansatz(weights1)\n",
    "    block.embedding(inputs2)\n",
    "    block.ansatz(weights2)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(8)]\n",
    "\n",
    "def quantum_layer(inputs1,inputs2, weights1,weights2):\n",
    "    return quantum_circuit(inputs1,inputs2, weights1,weights2)\n",
    "\n",
    "n_qubits = 8\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "\n",
    "def generate_tensor(seed, size):\n",
    "    \"\"\"\n",
    "    주어진 시드와 크기에 맞게 torch.Tensor를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        seed (int): 시드 값\n",
    "        size (tuple): 생성할 텐서의 크기\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 생성된 텐서\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    return torch.randn(size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RNN_layer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,num_layers):\n",
    "        \"\"\"_RNN layer 만든거_\n",
    "\n",
    "        Args:\n",
    "            input_size (_int_): _input feature의 개수_\n",
    "            output_size (_int_): _output feature의 개수_\n",
    "            num_layers (_int_): _필요한 RNN layer 수_\n",
    "        \"\"\"\n",
    "        super(RNN_layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layer = num_layers\n",
    "        self.cls_layer = nn.Sequential(nn.Linear(8,16),nn.ReLU(),nn.Linear(16,1))\n",
    "        ## QNE 수행할 Linear layer\n",
    "        for i in range(num_layers):\n",
    "            setattr(self,f'QNE_layer_{i}',KAN([input_size,2*input_size+1,input_size*2-1],grid=1))\n",
    "        \n",
    "        ## Ansatz parameter\n",
    "        self.ansatz_params_1 = nn.Parameter(torch.rand([24],dtype = torch.float32),requires_grad=True)\n",
    "        self.ansatz_params_2 = nn.Parameter(torch.rand([24],dtype = torch.float32),requires_grad=True)\n",
    "        self.rnn_layer = quantum_layer\n",
    "\n",
    "        \n",
    "    def forward(self,inputs,return_hidden_list = False):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            inputs (_torch tensor_): _(batch,seq_len,feature_size)_\n",
    "        \"\"\"\n",
    "        index = 0\n",
    "        \n",
    "        batch = inputs.shape[0]\n",
    "        seq_len = inputs.shape[1]\n",
    "        initial_t = generate_tensor(30,[inputs.shape[0],inputs.shape[2]]).float()\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "        ## inputs  = (seq_len,batch,feature_size)\n",
    "        input = getattr(self,f'QNE_layer_{index}')(inputs[0])\n",
    "        index+=1\n",
    "        hidden = torch.stack(self.rnn_layer(input,initial_t,self.ansatz_params_1,self.ansatz_params_2),dim=1).float()\n",
    "        hidden = hidden.to(torch.float32)\n",
    "        if return_hidden_list:\n",
    "            hidden_list = hidden\n",
    "        for input in inputs[1:]:\n",
    "            input = getattr(self,f'QNE_layer_{index}')(input)\n",
    "            index+=1\n",
    "            hidden = torch.stack(self.rnn_layer(input,hidden,self.ansatz_params_1,self.ansatz_params_2),dim=1).float()\n",
    "            hidden = hidden.to(torch.float32)\n",
    "            if return_hidden_list:\n",
    "                hidden_list = torch.concat([hidden_list,hidden])\n",
    "        if return_hidden_list:\n",
    "            hidden_list = torch.reshape(hidden_list,[batch,seq_len,-1])\n",
    "            return hidden_list\n",
    "        return self.cls_layer(hidden)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_frame = pd.read_csv('./dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = ['nat_demand','T2M_toc','QV2M_toc','TQL_toc','QV2M_san','W2M_san','QV2M_dav','W2M_dav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = data_frame[key_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set  = torch.tensor(np.array(feature1))[:4200]\n",
    "data_set = torch.reshape(data_set,[-1,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [7*i+8 for i in range(400) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_set[:400]\n",
    "train_label = torch.tensor(np.array(data_frame['nat_demand'].iloc[label_list]))\n",
    "test_data = data_set[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 1263785.2500\n",
      "Epoch [2/500], Loss: 1261866.0000\n",
      "Epoch [3/500], Loss: 1259216.1250\n",
      "Epoch [4/500], Loss: 1254899.3750\n",
      "Epoch [5/500], Loss: 1249093.3750\n",
      "Epoch [6/500], Loss: 1240819.0000\n",
      "Epoch [7/500], Loss: 1230358.5000\n",
      "Epoch [8/500], Loss: 1217994.7500\n",
      "Epoch [9/500], Loss: 1196244.7500\n",
      "Epoch [10/500], Loss: 1178676.2500\n",
      "Epoch [11/500], Loss: 1145298.1250\n",
      "Epoch [12/500], Loss: 1118004.1250\n",
      "Epoch [13/500], Loss: 1087227.1250\n",
      "Epoch [14/500], Loss: 1063077.8750\n",
      "Epoch [15/500], Loss: 1026392.8750\n",
      "Epoch [16/500], Loss: 982157.5000\n",
      "Epoch [17/500], Loss: 939556.7500\n",
      "Epoch [18/500], Loss: 897312.6875\n",
      "Epoch [19/500], Loss: 845707.8125\n",
      "Epoch [20/500], Loss: 722240.1250\n",
      "Epoch [21/500], Loss: 557006.8750\n",
      "Epoch [22/500], Loss: 388910.4688\n",
      "Epoch [23/500], Loss: 322447.2188\n",
      "Epoch [24/500], Loss: 248862.9219\n",
      "Epoch [25/500], Loss: 188042.4375\n",
      "Epoch [26/500], Loss: 140311.4844\n",
      "Epoch [27/500], Loss: 134089.9375\n",
      "Epoch [28/500], Loss: 117622.2734\n",
      "Epoch [29/500], Loss: 76857.1172\n",
      "Epoch [30/500], Loss: 108881.0781\n",
      "Epoch [31/500], Loss: 72013.5938\n",
      "Epoch [32/500], Loss: 92942.6328\n",
      "Epoch [33/500], Loss: 67770.7891\n",
      "Epoch [34/500], Loss: 74148.2266\n",
      "Epoch [35/500], Loss: 67411.4297\n",
      "Epoch [36/500], Loss: 47809.6016\n",
      "Epoch [37/500], Loss: 49047.1523\n",
      "Epoch [38/500], Loss: 50695.0391\n",
      "Epoch [39/500], Loss: 41604.8281\n",
      "Epoch [40/500], Loss: 45891.5391\n",
      "Epoch [41/500], Loss: 40722.7891\n",
      "Epoch [42/500], Loss: 38144.2383\n",
      "Epoch [43/500], Loss: 44201.2305\n",
      "Epoch [44/500], Loss: 39228.6016\n",
      "Epoch [45/500], Loss: 41889.2344\n",
      "Epoch [46/500], Loss: 42671.8320\n",
      "Epoch [47/500], Loss: 38390.1445\n",
      "Epoch [48/500], Loss: 40358.0078\n",
      "Epoch [49/500], Loss: 39847.5977\n",
      "Epoch [50/500], Loss: 37777.4922\n",
      "Epoch [51/500], Loss: 39261.7383\n",
      "Epoch [52/500], Loss: 39200.0039\n",
      "Epoch [53/500], Loss: 37814.0391\n",
      "Epoch [54/500], Loss: 37637.3281\n",
      "Epoch [55/500], Loss: 38327.7617\n",
      "Epoch [56/500], Loss: 38019.6797\n",
      "Epoch [57/500], Loss: 37251.6289\n",
      "Epoch [58/500], Loss: 37308.2812\n",
      "Epoch [59/500], Loss: 37701.2070\n",
      "Epoch [60/500], Loss: 37750.5000\n",
      "Epoch [61/500], Loss: 37140.3789\n",
      "Epoch [62/500], Loss: 37152.4570\n",
      "Epoch [63/500], Loss: 37435.3320\n",
      "Epoch [64/500], Loss: 37371.0820\n",
      "Epoch [65/500], Loss: 37130.3047\n",
      "Epoch [66/500], Loss: 37045.1914\n",
      "Epoch [67/500], Loss: 37293.6055\n",
      "Epoch [68/500], Loss: 37244.5391\n",
      "Epoch [69/500], Loss: 37012.3359\n",
      "Epoch [70/500], Loss: 37021.8398\n",
      "Epoch [71/500], Loss: 37174.4688\n",
      "Epoch [72/500], Loss: 37115.6094\n",
      "Epoch [73/500], Loss: 36987.8477\n",
      "Epoch [74/500], Loss: 37020.0977\n",
      "Epoch [75/500], Loss: 37095.3203\n",
      "Epoch [76/500], Loss: 36999.3203\n",
      "Epoch [77/500], Loss: 36954.1406\n",
      "Epoch [78/500], Loss: 37029.6250\n",
      "Epoch [79/500], Loss: 37045.1523\n",
      "Epoch [80/500], Loss: 36953.0039\n",
      "Epoch [81/500], Loss: 36945.1094\n",
      "Epoch [82/500], Loss: 36971.7891\n",
      "Epoch [83/500], Loss: 36971.4844\n",
      "Epoch [84/500], Loss: 36933.1484\n",
      "Epoch [85/500], Loss: 36924.2539\n",
      "Epoch [86/500], Loss: 36961.1445\n",
      "Epoch [87/500], Loss: 36937.8555\n",
      "Epoch [88/500], Loss: 36912.3984\n",
      "Epoch [89/500], Loss: 36928.5039\n",
      "Epoch [90/500], Loss: 36937.0195\n",
      "Epoch [91/500], Loss: 36918.7930\n",
      "Epoch [92/500], Loss: 36898.1719\n",
      "Epoch [93/500], Loss: 36911.3359\n",
      "Epoch [94/500], Loss: 36920.3555\n",
      "Epoch [95/500], Loss: 36907.2461\n",
      "Epoch [96/500], Loss: 36904.9219\n",
      "Epoch [97/500], Loss: 36903.8594\n",
      "Epoch [98/500], Loss: 36902.9648\n",
      "Epoch [99/500], Loss: 36898.4258\n",
      "Epoch [100/500], Loss: 36901.2305\n",
      "Epoch [101/500], Loss: 36898.3438\n",
      "Epoch [102/500], Loss: 36899.2500\n",
      "Epoch [103/500], Loss: 36894.3086\n",
      "Epoch [104/500], Loss: 36897.7500\n",
      "Epoch [105/500], Loss: 36899.2461\n",
      "Epoch [106/500], Loss: 36892.1016\n",
      "Epoch [107/500], Loss: 36895.0859\n",
      "Epoch [108/500], Loss: 36895.2852\n",
      "Epoch [109/500], Loss: 36892.4375\n",
      "Epoch [110/500], Loss: 36889.3398\n",
      "Epoch [111/500], Loss: 36889.0742\n",
      "Epoch [112/500], Loss: 36890.8750\n",
      "Epoch [113/500], Loss: 36890.8867\n",
      "Epoch [114/500], Loss: 36887.5352\n",
      "Epoch [115/500], Loss: 36890.4844\n",
      "Epoch [116/500], Loss: 36888.0781\n",
      "Epoch [117/500], Loss: 36889.2383\n",
      "Epoch [118/500], Loss: 36889.5859\n",
      "Epoch [119/500], Loss: 36885.1953\n",
      "Epoch [120/500], Loss: 36885.4961\n",
      "Epoch [121/500], Loss: 36886.0312\n",
      "Epoch [122/500], Loss: 36885.8555\n",
      "Epoch [123/500], Loss: 36886.4961\n",
      "Epoch [124/500], Loss: 36885.7852\n",
      "Epoch [125/500], Loss: 36884.0430\n",
      "Epoch [126/500], Loss: 36883.5781\n",
      "Epoch [127/500], Loss: 36884.7930\n",
      "Epoch [128/500], Loss: 36884.0195\n",
      "Epoch [129/500], Loss: 36882.5898\n",
      "Epoch [130/500], Loss: 36882.5547\n",
      "Epoch [131/500], Loss: 36882.9102\n",
      "Epoch [132/500], Loss: 36881.9141\n",
      "Epoch [133/500], Loss: 36882.8555\n",
      "Epoch [134/500], Loss: 36881.9570\n",
      "Epoch [135/500], Loss: 36882.1680\n",
      "Epoch [136/500], Loss: 36883.7773\n",
      "Epoch [137/500], Loss: 36880.7539\n",
      "Epoch [138/500], Loss: 36882.4062\n",
      "Epoch [139/500], Loss: 36881.6289\n",
      "Epoch [140/500], Loss: 36881.3242\n",
      "Epoch [141/500], Loss: 36879.8477\n",
      "Epoch [142/500], Loss: 36879.0352\n",
      "Epoch [143/500], Loss: 36880.8164\n",
      "Epoch [144/500], Loss: 36880.8828\n",
      "Epoch [145/500], Loss: 36882.2734\n",
      "Epoch [146/500], Loss: 36884.1328\n",
      "Epoch [147/500], Loss: 36880.9102\n",
      "Epoch [148/500], Loss: 36879.1172\n",
      "Epoch [149/500], Loss: 36880.2734\n",
      "Epoch [150/500], Loss: 36879.3203\n",
      "Epoch [151/500], Loss: 36879.7383\n",
      "Epoch [152/500], Loss: 36878.3281\n",
      "Epoch [153/500], Loss: 36879.4883\n",
      "Epoch [154/500], Loss: 36880.1484\n",
      "Epoch [155/500], Loss: 36878.0781\n",
      "Epoch [156/500], Loss: 36879.0742\n",
      "Epoch [157/500], Loss: 36881.3047\n",
      "Epoch [158/500], Loss: 36878.9062\n",
      "Epoch [159/500], Loss: 36879.7578\n",
      "Epoch [160/500], Loss: 36879.4648\n",
      "Epoch [161/500], Loss: 36878.3984\n",
      "Epoch [162/500], Loss: 36877.9922\n",
      "Epoch [163/500], Loss: 36877.6953\n",
      "Epoch [164/500], Loss: 36877.8047\n",
      "Epoch [165/500], Loss: 36880.1523\n",
      "Epoch [166/500], Loss: 36878.9414\n",
      "Epoch [167/500], Loss: 36879.1562\n",
      "Epoch [168/500], Loss: 36878.9062\n",
      "Epoch [169/500], Loss: 36876.8633\n",
      "Epoch [170/500], Loss: 36877.6914\n",
      "Epoch [171/500], Loss: 36876.4336\n",
      "Epoch [172/500], Loss: 36878.5273\n",
      "Epoch [173/500], Loss: 36876.7695\n",
      "Epoch [174/500], Loss: 36876.3281\n",
      "Epoch [175/500], Loss: 36875.6562\n",
      "Epoch [176/500], Loss: 36876.3438\n",
      "Epoch [177/500], Loss: 36877.4062\n",
      "Epoch [178/500], Loss: 36876.6133\n",
      "Epoch [179/500], Loss: 36876.5977\n",
      "Epoch [180/500], Loss: 36875.6055\n",
      "Epoch [181/500], Loss: 36876.0195\n",
      "Epoch [182/500], Loss: 36875.3867\n",
      "Epoch [183/500], Loss: 36876.5391\n",
      "Epoch [184/500], Loss: 36876.1992\n",
      "Epoch [185/500], Loss: 36875.4766\n",
      "Epoch [186/500], Loss: 36875.5625\n",
      "Epoch [187/500], Loss: 36875.2109\n",
      "Epoch [188/500], Loss: 36875.8750\n",
      "Epoch [189/500], Loss: 36874.8203\n",
      "Epoch [190/500], Loss: 36875.3164\n",
      "Epoch [191/500], Loss: 36875.0273\n",
      "Epoch [192/500], Loss: 36874.7305\n",
      "Epoch [193/500], Loss: 36874.6680\n",
      "Epoch [194/500], Loss: 36875.1250\n",
      "Epoch [195/500], Loss: 36874.9102\n",
      "Epoch [196/500], Loss: 36875.7734\n",
      "Epoch [197/500], Loss: 36875.4688\n",
      "Epoch [198/500], Loss: 36875.0234\n",
      "Epoch [199/500], Loss: 36874.5391\n",
      "Epoch [200/500], Loss: 36874.3672\n",
      "Epoch [201/500], Loss: 36874.5742\n",
      "Epoch [202/500], Loss: 36874.4102\n",
      "Epoch [203/500], Loss: 36874.4023\n",
      "Epoch [204/500], Loss: 36874.1992\n",
      "Epoch [205/500], Loss: 36873.9727\n",
      "Epoch [206/500], Loss: 36874.1133\n",
      "Epoch [207/500], Loss: 36873.9414\n",
      "Epoch [208/500], Loss: 36874.1211\n",
      "Epoch [209/500], Loss: 36874.3750\n",
      "Epoch [210/500], Loss: 36874.1250\n",
      "Epoch [211/500], Loss: 36874.2227\n",
      "Epoch [212/500], Loss: 36874.6055\n",
      "Epoch [213/500], Loss: 36873.8047\n",
      "Epoch [214/500], Loss: 36875.0078\n",
      "Epoch [215/500], Loss: 36874.5586\n",
      "Epoch [216/500], Loss: 36875.0000\n",
      "Epoch [217/500], Loss: 36874.5195\n",
      "Epoch [218/500], Loss: 36875.0820\n",
      "Epoch [219/500], Loss: 36874.1445\n",
      "Epoch [220/500], Loss: 36875.0977\n",
      "Epoch [221/500], Loss: 36874.9570\n",
      "Epoch [222/500], Loss: 36874.6641\n",
      "Epoch [223/500], Loss: 36874.5586\n",
      "Epoch [224/500], Loss: 36874.4062\n",
      "Epoch [225/500], Loss: 36874.4102\n",
      "Epoch [226/500], Loss: 36874.2852\n",
      "Epoch [227/500], Loss: 36874.2617\n",
      "Epoch [228/500], Loss: 36874.1992\n",
      "Epoch [229/500], Loss: 36874.2148\n",
      "Epoch [230/500], Loss: 36874.5195\n",
      "Epoch [231/500], Loss: 36874.2578\n",
      "Epoch [232/500], Loss: 36874.2852\n",
      "Epoch [233/500], Loss: 36873.7109\n",
      "Epoch [234/500], Loss: 36873.7383\n",
      "Epoch [235/500], Loss: 36873.6914\n",
      "Epoch [236/500], Loss: 36873.6250\n",
      "Epoch [237/500], Loss: 36873.7188\n",
      "Epoch [238/500], Loss: 36873.6055\n",
      "Epoch [239/500], Loss: 36873.5352\n",
      "Epoch [240/500], Loss: 36873.4297\n",
      "Epoch [241/500], Loss: 36873.4727\n",
      "Epoch [242/500], Loss: 36873.5898\n",
      "Epoch [243/500], Loss: 36873.9961\n",
      "Epoch [244/500], Loss: 36873.2695\n",
      "Epoch [245/500], Loss: 36873.5703\n",
      "Epoch [246/500], Loss: 36873.3438\n",
      "Epoch [247/500], Loss: 36873.6211\n",
      "Epoch [248/500], Loss: 36873.2148\n",
      "Epoch [249/500], Loss: 36873.5859\n",
      "Epoch [250/500], Loss: 36873.1914\n",
      "Epoch [251/500], Loss: 36873.4453\n",
      "Epoch [252/500], Loss: 36873.1406\n",
      "Epoch [253/500], Loss: 36873.3320\n",
      "Epoch [254/500], Loss: 36873.1680\n",
      "Epoch [255/500], Loss: 36873.2656\n",
      "Epoch [256/500], Loss: 36873.2305\n",
      "Epoch [257/500], Loss: 36873.1055\n",
      "Epoch [258/500], Loss: 36873.2109\n",
      "Epoch [259/500], Loss: 36873.0664\n",
      "Epoch [260/500], Loss: 36873.0000\n",
      "Epoch [261/500], Loss: 36873.0703\n",
      "Epoch [262/500], Loss: 36872.9258\n",
      "Epoch [263/500], Loss: 36873.0312\n",
      "Epoch [264/500], Loss: 36872.7656\n",
      "Epoch [265/500], Loss: 36873.0117\n",
      "Epoch [266/500], Loss: 36872.7695\n",
      "Epoch [267/500], Loss: 36872.8828\n",
      "Epoch [268/500], Loss: 36872.7422\n",
      "Epoch [269/500], Loss: 36872.8281\n",
      "Epoch [270/500], Loss: 36872.7148\n",
      "Epoch [271/500], Loss: 36872.7734\n",
      "Epoch [272/500], Loss: 36872.6758\n",
      "Epoch [273/500], Loss: 36872.7539\n",
      "Epoch [274/500], Loss: 36872.6562\n",
      "Epoch [275/500], Loss: 36872.6289\n",
      "Epoch [276/500], Loss: 36872.6641\n",
      "Epoch [277/500], Loss: 36872.5977\n",
      "Epoch [278/500], Loss: 36872.6406\n",
      "Epoch [279/500], Loss: 36872.5977\n",
      "Epoch [280/500], Loss: 36872.5898\n",
      "Epoch [281/500], Loss: 36872.5977\n",
      "Epoch [282/500], Loss: 36872.5547\n",
      "Epoch [283/500], Loss: 36872.5586\n",
      "Epoch [284/500], Loss: 36872.5547\n",
      "Epoch [285/500], Loss: 36872.5195\n",
      "Epoch [286/500], Loss: 36872.5156\n",
      "Epoch [287/500], Loss: 36872.5234\n",
      "Epoch [288/500], Loss: 36872.4922\n",
      "Epoch [289/500], Loss: 36872.4805\n",
      "Epoch [290/500], Loss: 36872.4922\n",
      "Epoch [291/500], Loss: 36872.4688\n",
      "Epoch [292/500], Loss: 36872.4531\n",
      "Epoch [293/500], Loss: 36872.4688\n",
      "Epoch [294/500], Loss: 36872.4492\n",
      "Epoch [295/500], Loss: 36872.4336\n",
      "Epoch [296/500], Loss: 36872.4336\n",
      "Epoch [297/500], Loss: 36872.4297\n",
      "Epoch [298/500], Loss: 36872.4141\n",
      "Epoch [299/500], Loss: 36872.4102\n",
      "Epoch [300/500], Loss: 36872.4023\n",
      "Epoch [301/500], Loss: 36872.3906\n",
      "Epoch [302/500], Loss: 36872.3828\n",
      "Epoch [303/500], Loss: 36872.3828\n",
      "Epoch [304/500], Loss: 36872.3672\n",
      "Epoch [305/500], Loss: 36872.3633\n",
      "Epoch [306/500], Loss: 36872.3633\n",
      "Epoch [307/500], Loss: 36872.3516\n",
      "Epoch [308/500], Loss: 36872.3438\n",
      "Epoch [309/500], Loss: 36872.3398\n",
      "Epoch [310/500], Loss: 36872.3320\n",
      "Epoch [311/500], Loss: 36872.3203\n",
      "Epoch [312/500], Loss: 36872.3203\n",
      "Epoch [313/500], Loss: 36872.3125\n",
      "Epoch [314/500], Loss: 36872.3086\n",
      "Epoch [315/500], Loss: 36872.3008\n",
      "Epoch [316/500], Loss: 36872.2930\n",
      "Epoch [317/500], Loss: 36872.2930\n",
      "Epoch [318/500], Loss: 36872.2812\n",
      "Epoch [319/500], Loss: 36872.2734\n",
      "Epoch [320/500], Loss: 36872.2695\n",
      "Epoch [321/500], Loss: 36872.2656\n",
      "Epoch [322/500], Loss: 36872.2578\n",
      "Epoch [323/500], Loss: 36872.2500\n",
      "Epoch [324/500], Loss: 36872.2461\n",
      "Epoch [325/500], Loss: 36872.2383\n",
      "Epoch [326/500], Loss: 36872.2305\n",
      "Epoch [327/500], Loss: 36872.2266\n",
      "Epoch [328/500], Loss: 36872.2227\n",
      "Epoch [329/500], Loss: 36872.2148\n",
      "Epoch [330/500], Loss: 36872.2109\n",
      "Epoch [331/500], Loss: 36872.2031\n",
      "Epoch [332/500], Loss: 36872.1992\n",
      "Epoch [333/500], Loss: 36872.1953\n",
      "Epoch [334/500], Loss: 36872.1875\n",
      "Epoch [335/500], Loss: 36872.1797\n",
      "Epoch [336/500], Loss: 36872.1719\n",
      "Epoch [337/500], Loss: 36872.1680\n",
      "Epoch [338/500], Loss: 36872.1680\n",
      "Epoch [339/500], Loss: 36872.1523\n",
      "Epoch [340/500], Loss: 36872.1523\n",
      "Epoch [341/500], Loss: 36872.1484\n",
      "Epoch [342/500], Loss: 36872.1406\n",
      "Epoch [343/500], Loss: 36872.1367\n",
      "Epoch [344/500], Loss: 36872.1328\n",
      "Epoch [345/500], Loss: 36872.1289\n",
      "Epoch [346/500], Loss: 36872.1211\n",
      "Epoch [347/500], Loss: 36872.1133\n",
      "Epoch [348/500], Loss: 36872.1133\n",
      "Epoch [349/500], Loss: 36872.1094\n",
      "Epoch [350/500], Loss: 36872.1016\n",
      "Epoch [351/500], Loss: 36872.0977\n",
      "Epoch [352/500], Loss: 36872.0898\n",
      "Epoch [353/500], Loss: 36872.0898\n",
      "Epoch [354/500], Loss: 36872.0820\n",
      "Epoch [355/500], Loss: 36872.0781\n",
      "Epoch [356/500], Loss: 36872.0781\n",
      "Epoch [357/500], Loss: 36872.0703\n",
      "Epoch [358/500], Loss: 36872.0664\n",
      "Epoch [359/500], Loss: 36872.0625\n",
      "Epoch [360/500], Loss: 36872.0586\n",
      "Epoch [361/500], Loss: 36872.0508\n",
      "Epoch [362/500], Loss: 36872.0469\n",
      "Epoch [363/500], Loss: 36872.0430\n",
      "Epoch [364/500], Loss: 36872.0430\n",
      "Epoch [365/500], Loss: 36872.0391\n",
      "Epoch [366/500], Loss: 36872.0352\n",
      "Epoch [367/500], Loss: 36872.0312\n",
      "Epoch [368/500], Loss: 36872.0273\n",
      "Epoch [369/500], Loss: 36872.0195\n",
      "Epoch [370/500], Loss: 36872.0195\n",
      "Epoch [371/500], Loss: 36872.0117\n",
      "Epoch [372/500], Loss: 36872.0078\n",
      "Epoch [373/500], Loss: 36872.0039\n",
      "Epoch [374/500], Loss: 36872.0000\n",
      "Epoch [375/500], Loss: 36872.0000\n",
      "Epoch [376/500], Loss: 36871.9922\n",
      "Epoch [377/500], Loss: 36871.9922\n",
      "Epoch [378/500], Loss: 36871.9883\n",
      "Epoch [379/500], Loss: 36871.9805\n",
      "Epoch [380/500], Loss: 36871.9766\n",
      "Epoch [381/500], Loss: 36871.9766\n",
      "Epoch [382/500], Loss: 36871.9727\n",
      "Epoch [383/500], Loss: 36871.9688\n",
      "Epoch [384/500], Loss: 36871.9648\n",
      "Epoch [385/500], Loss: 36871.9648\n",
      "Epoch [386/500], Loss: 36871.9609\n",
      "Epoch [387/500], Loss: 36871.9531\n",
      "Epoch [388/500], Loss: 36871.9531\n",
      "Epoch [389/500], Loss: 36871.9531\n",
      "Epoch [390/500], Loss: 36871.9414\n",
      "Epoch [391/500], Loss: 36871.9414\n",
      "Epoch [392/500], Loss: 36871.9414\n",
      "Epoch [393/500], Loss: 36871.9375\n",
      "Epoch [394/500], Loss: 36871.9336\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_rnn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[129], line 35\u001b[0m, in \u001b[0;36mtrain_rnn_layer\u001b[1;34m(inputs, targets)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\EMT\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rnn_layer(train_data.float() ,train_label.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Define the training loop\n",
    "\n",
    "\n",
    "\n",
    "def train_rnn_layer(inputs,targets,model):\n",
    "    # Parameters\n",
    "    input_size = 8\n",
    "    output_size = 8\n",
    "    num_layers = 7\n",
    "    num_epochs = 500\n",
    "    learning_rate = 0.2\n",
    "\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Dummy data for training\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.to(torch.float32)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_layer(8, 8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RNN_layer(\n",
       "  (cls_layer): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (QNE_layer_0): KAN(\n",
       "    (biases): ModuleList(\n",
       "      (0): Linear(in_features=17, out_features=1, bias=False)\n",
       "      (1): Linear(in_features=15, out_features=1, bias=False)\n",
       "    )\n",
       "    (act_fun): ModuleList(\n",
       "      (0-1): 2 x KANLayer(\n",
       "        (base_fun): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (base_fun): SiLU()\n",
       "    (symbolic_fun): ModuleList(\n",
       "      (0-1): 2 x Symbolic_KANLayer()\n",
       "    )\n",
       "  )\n",
       "  (QNE_layer_1): KAN(\n",
       "    (biases): ModuleList(\n",
       "      (0): Linear(in_features=17, out_features=1, bias=False)\n",
       "      (1): Linear(in_features=15, out_features=1, bias=False)\n",
       "    )\n",
       "    (act_fun): ModuleList(\n",
       "      (0-1): 2 x KANLayer(\n",
       "        (base_fun): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (base_fun): SiLU()\n",
       "    (symbolic_fun): ModuleList(\n",
       "      (0-1): 2 x Symbolic_KANLayer()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6167, 0.8152, 0.0657, 0.0846, 0.7246, 0.6801, 0.0346, 0.2212, 0.0425,\n",
       "        0.1418, 0.2744, 0.9138, 0.4005, 0.7872, 0.9748, 0.7279, 0.0524, 0.7516,\n",
       "        0.6907, 0.8875, 0.0068, 0.8075, 0.9093, 0.0348], requires_grad=True)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ansatz_params_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 10, 8])\n",
      "Output: tensor([[[ 1.5881e-01, -3.2125e-01, -1.3448e-01, -3.6123e-01, -5.0410e-02,\n",
      "          -1.7403e-02, -1.2152e-01,  4.4982e-02],\n",
      "         [ 2.0169e-02,  6.4169e-01,  1.3500e-01,  3.3601e-01,  9.9510e-02,\n",
      "           5.6186e-02,  8.2669e-02, -3.9853e-02],\n",
      "         [ 5.6169e-01,  5.4541e-01,  2.0663e-01,  1.7761e-01, -6.0539e-02,\n",
      "           6.4748e-02,  4.9791e-02, -6.1873e-02],\n",
      "         [ 1.3180e-01, -2.9873e-01,  2.2480e-01,  3.9494e-01, -5.8878e-02,\n",
      "           5.6205e-02,  1.7944e-01, -1.2226e-01],\n",
      "         [ 9.4118e-02,  4.9910e-02,  3.6760e-01,  1.9947e-01,  9.7139e-02,\n",
      "          -1.7423e-02,  5.7610e-02,  8.2776e-03],\n",
      "         [ 3.3404e-02, -3.3880e-01, -1.9442e-01,  8.0139e-02,  1.5477e-02,\n",
      "          -2.8924e-02,  3.4318e-02,  6.9795e-03],\n",
      "         [ 7.9341e-01,  8.2112e-01,  2.8879e-01,  5.4817e-01, -2.3316e-01,\n",
      "           1.3366e-01,  2.5388e-01, -2.2767e-01],\n",
      "         [-7.6532e-02,  7.1550e-02,  2.8889e-01, -1.1167e-01, -5.4760e-02,\n",
      "           4.9748e-02, -4.9138e-02, -1.8776e-02],\n",
      "         [ 2.2316e-01,  2.3104e-01, -1.3433e-01,  1.0220e-01,  8.2480e-02,\n",
      "           2.6933e-02,  2.1537e-02, -1.0200e-02],\n",
      "         [ 1.5478e-01,  3.5867e-01, -4.6981e-02,  2.3185e-02,  7.9836e-02,\n",
      "           2.6762e-02, -2.0262e-02,  9.6809e-03]],\n",
      "\n",
      "        [[-1.6022e-01, -2.1823e-01,  3.0257e-02, -1.4184e-01,  1.5028e-02,\n",
      "          -2.9636e-02, -5.6073e-02,  4.3233e-02],\n",
      "         [-1.5169e-01, -3.3692e-01, -1.0303e-02, -1.2165e-01, -7.3412e-02,\n",
      "          -4.1663e-02, -3.0567e-02,  2.5105e-02],\n",
      "         [ 2.1511e-01, -4.2139e-01,  2.0871e-01,  1.0900e-01, -1.5147e-01,\n",
      "           5.4566e-02,  6.1676e-02, -8.4303e-02],\n",
      "         [ 7.6192e-01,  4.5391e-01, -6.5420e-02,  7.5198e-02,  2.1023e-02,\n",
      "           2.2725e-02,  3.2942e-02, -2.6856e-02],\n",
      "         [-5.8324e-02, -1.7424e-01,  2.0613e-02,  4.3721e-02, -1.9372e-02,\n",
      "           1.5453e-02,  1.7324e-02, -1.9249e-02],\n",
      "         [ 3.0883e-02,  4.1325e-02, -2.5693e-02, -3.3480e-01,  5.3770e-02,\n",
      "          -5.6605e-02, -1.0343e-01,  7.3134e-02],\n",
      "         [ 8.7635e-01, -1.6785e-01, -1.9898e-01,  5.6901e-02,  7.6685e-02,\n",
      "          -3.8364e-02,  1.9168e-02,  2.8213e-02],\n",
      "         [ 8.3652e-01, -1.1847e-01, -3.3499e-02,  2.6708e-03,  1.7933e-02,\n",
      "          -1.7021e-02, -8.6170e-04,  1.4502e-02],\n",
      "         [-1.3587e-01,  6.9719e-02,  1.3091e-01,  5.1726e-02, -6.3665e-02,\n",
      "           3.8193e-02,  1.9286e-02, -3.9791e-02],\n",
      "         [ 4.2255e-01,  1.4570e-01, -1.2817e-01, -3.5640e-02,  7.2624e-02,\n",
      "          -2.1454e-02, -2.0362e-02,  3.2715e-02]],\n",
      "\n",
      "        [[ 7.9428e-01,  2.6191e-01, -1.2088e-01,  8.5067e-04,  1.0611e-01,\n",
      "          -1.1148e-02, -1.3308e-02,  2.9541e-02],\n",
      "         [-2.3408e-01,  5.3602e-01,  2.6373e-01,  9.6444e-02, -1.3982e-01,\n",
      "           7.9125e-02,  5.8501e-02, -9.8984e-02],\n",
      "         [-1.9830e-01, -5.2558e-02,  4.0128e-01,  2.3356e-01, -2.6547e-01,\n",
      "           1.0340e-01,  1.2348e-01, -1.5838e-01],\n",
      "         [ 2.9475e-01, -1.4998e-01,  1.9124e-01, -5.3125e-01,  3.1890e-02,\n",
      "          -4.7610e-02, -1.9262e-01,  1.0333e-01],\n",
      "         [ 2.2184e-01,  5.9608e-01,  1.6427e-01,  1.2381e-01, -8.3331e-02,\n",
      "           7.0107e-02,  5.8160e-02, -8.2325e-02],\n",
      "         [ 4.2893e-02,  2.4766e-01, -8.0775e-02, -3.3155e-02,  6.9523e-02,\n",
      "          -1.2007e-03, -1.7826e-02,  1.7178e-02],\n",
      "         [ 1.2991e-01,  1.6641e-01, -4.7600e-02, -1.7175e-01,  1.6080e-02,\n",
      "          -3.7525e-02, -4.1131e-02,  3.2891e-02],\n",
      "         [ 6.9448e-01, -9.3505e-02, -2.2317e-01, -9.7736e-02,  1.1241e-01,\n",
      "          -4.7209e-02, -5.9032e-02,  7.4663e-02],\n",
      "         [ 1.7623e-01, -2.1346e-02,  1.7409e-01, -3.7124e-01,  2.7645e-02,\n",
      "          -4.9783e-02, -1.5059e-01,  9.6056e-02],\n",
      "         [ 1.5968e-01, -1.7312e-01,  4.1027e-02,  2.0491e-02, -2.0349e-02,\n",
      "           1.9449e-03,  1.0041e-02, -8.1688e-03]],\n",
      "\n",
      "        [[ 1.0693e-01, -2.0106e-01, -6.8178e-02, -5.2995e-01,  1.3876e-01,\n",
      "          -9.9998e-02, -1.9720e-01,  1.5415e-01],\n",
      "         [-2.9843e-03, -4.6041e-01,  2.0505e-01,  1.5528e-01, -1.5743e-01,\n",
      "           6.2886e-02,  7.5907e-02, -9.4531e-02],\n",
      "         [ 2.6608e-01,  4.1118e-02, -7.3857e-02, -1.1325e-02,  5.3012e-02,\n",
      "          -1.5947e-02, -5.0734e-03,  1.8811e-02],\n",
      "         [ 2.0819e-01,  2.3021e-01, -2.2450e-02, -8.4374e-02,  7.2905e-02,\n",
      "          -2.3676e-02, -5.7023e-02,  5.4876e-02],\n",
      "         [ 1.9498e-01, -5.8511e-01,  3.8194e-02, -3.0065e-01,  6.9195e-02,\n",
      "          -2.9798e-02, -1.1957e-01,  7.4105e-02],\n",
      "         [ 1.9705e-01, -3.3778e-01,  1.3951e-01, -4.8470e-01,  1.0749e-03,\n",
      "          -5.9809e-02, -1.6257e-01,  9.2174e-02],\n",
      "         [ 3.6684e-01, -1.2238e-01, -4.1245e-01, -1.0784e-01,  2.5912e-01,\n",
      "          -6.8325e-02, -9.6605e-02,  1.3224e-01],\n",
      "         [-3.3795e-01, -1.0816e-01,  2.9953e-01,  1.5327e-01, -2.0258e-01,\n",
      "           6.8642e-02,  8.4787e-02, -1.1047e-01],\n",
      "         [ 6.1010e-01,  7.0827e-01,  2.8035e-01,  2.8300e-01, -1.2672e-01,\n",
      "           1.2491e-01,  1.0867e-01, -1.3886e-01],\n",
      "         [ 1.9112e-02, -4.5844e-01,  7.5660e-02, -1.0108e-01,  2.7256e-02,\n",
      "           7.2053e-03, -3.9169e-02,  1.2983e-02]],\n",
      "\n",
      "        [[ 6.4405e-02,  3.1282e-02, -2.0418e-01, -1.4108e-01,  1.0557e-01,\n",
      "          -6.2096e-02, -6.1376e-02,  7.9296e-02],\n",
      "         [-2.3017e-02,  2.4113e-01, -3.3511e-01,  6.9165e-02,  1.9865e-01,\n",
      "          -2.8665e-02, -3.0309e-03,  5.3923e-02],\n",
      "         [ 6.9061e-01, -1.4974e-01, -2.0470e-01, -9.8869e-02,  9.4967e-02,\n",
      "          -5.6210e-02, -4.8358e-02,  7.0509e-02],\n",
      "         [ 9.3847e-02,  2.5537e-01,  2.4945e-02,  1.3589e-01, -2.9359e-02,\n",
      "           3.8518e-02,  5.0912e-02, -4.7767e-02],\n",
      "         [-1.5458e-01, -1.3324e-01, -3.2350e-01, -5.8528e-02,  1.4901e-01,\n",
      "          -8.2192e-02, -2.8929e-02,  8.5221e-02],\n",
      "         [ 2.6255e-01, -3.0189e-01, -2.4721e-02, -3.6938e-01,  1.1156e-01,\n",
      "          -5.5276e-02, -1.4297e-01,  1.0358e-01],\n",
      "         [ 4.6730e-01,  1.3299e-01,  3.7922e-01,  7.5703e-02, -2.3397e-01,\n",
      "           6.0406e-02,  7.1723e-02, -1.0996e-01],\n",
      "         [-1.5327e-01,  1.9432e-03,  4.5582e-02,  2.7189e-01, -1.7452e-02,\n",
      "           5.7984e-02,  8.0541e-02, -6.1939e-02],\n",
      "         [ 1.9002e-01, -8.0405e-02,  3.7813e-02, -5.2727e-01,  9.1348e-02,\n",
      "          -6.4331e-02, -1.9098e-01,  1.2055e-01],\n",
      "         [ 3.7141e-01, -1.3568e-01, -9.2800e-02, -3.3424e-01,  5.8829e-02,\n",
      "          -9.8225e-02, -1.1157e-01,  1.0797e-01]]], dtype=torch.float64,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "def test_rnn_layer():\n",
    "    # Parameters for the RNN layer\n",
    "    input_size = 8\n",
    "    output_size = 8\n",
    "    num_layers = 10\n",
    "\n",
    "    # Initialize the RNN layer\n",
    "    rnn_layer = RNN_layer(input_size, output_size, num_layers)\n",
    "\n",
    "    # Generate dummy input data\n",
    "    batch_size = 5\n",
    "    seq_len = 10\n",
    "    feature_size = input_size\n",
    "    inputs = torch.randn(batch_size, seq_len, feature_size)\n",
    "\n",
    "    # Forward pass through the RNN layer\n",
    "    output = rnn_layer(inputs,return_hidden_list = True)\n",
    "\n",
    "    # Print the output shape and the output itself\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output:\", output)\n",
    "\n",
    "test_rnn_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "class RNN_block:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "\n",
    "    \n",
    "    def embedding(self, params):\n",
    "        n = (self.input_size + 1) // 2\n",
    "        for i in range(n):\n",
    "            qml.Hadamard(i)\n",
    "            qml.RZ(2.0 * params[:,i], i)\n",
    "        \n",
    "        for i in range(n - 1):\n",
    "            qml.IsingZZ(2.0 * params[:,n + i] ,[i , i + 1])\n",
    "    \n",
    "    def ansatz(self, params, all_entangled = False):\n",
    "        # Length of Params : 3 * num_qubit\n",
    "        n = self.input_size\n",
    "        for i in range(n):\n",
    "            qml.RX(params[3 * i], i)\n",
    "            qml.RY(params[3 * i + 1], i)\n",
    "            qml.RZ(params[3 * i + 2], i)\n",
    "        for i in range(n - 1):\n",
    "            qml.CNOT([i, i + 1])\n",
    "        if all_entangled:\n",
    "            qml.CNOT([n - 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 8\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs1,inputs2, weights1,weights2):\n",
    "    block = RNN_block(8)\n",
    "    block.embedding(inputs1)\n",
    "    block.ansatz(weights1)\n",
    "    block.embedding(inputs2)\n",
    "    block.ansatz(weights2)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(8)]\n",
    "def quantum_layer(inputs1,inputs2, weights1,weights2):\n",
    "    return quantum_circuit(inputs1,inputs2, weights1,weights2)\n",
    "\n",
    "torch.stack(quantum_circuit(torch.rand([4,15]),torch.rand([4,15]),torch.rand([24]),torch.rand([24])),dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
