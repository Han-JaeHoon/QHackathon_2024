{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from torch import nn\n",
    "from kan import KAN\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "n_qubits = 8\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs, weights):\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(inputs[i], wires=i)\n",
    "    qml.layer(lambda w: [qml.Rot(w[i, 0], w[i, 1], w[i, 2], wires=i) for i in range(n_qubits)], weights.shape[0])(weights)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "def quantum_layer(inputs, weights):\n",
    "    return quantum_circuit(inputs, weights)\n",
    "\n",
    "def generate_tensor(seed, size):\n",
    "    \"\"\"\n",
    "    주어진 시드와 크기에 맞게 torch.Tensor를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        seed (int): 시드 값\n",
    "        size (tuple): 생성할 텐서의 크기\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 생성된 텐서\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    return torch.randn(size)\n",
    "\n",
    "class RNN_block(nn.Module):\n",
    "    def __init__(self, input_size, **kwargs):\n",
    "        super(RNN_block, self).__init__()\n",
    "        self.linear = nn.Linear(input_size*2,input_size)\n",
    "    def forward(self,input,hidden):\n",
    "        hidden = torch.concat([input,hidden],dim=1)\n",
    "        output = self.linear(hidden)\n",
    "        return output,output[:,0]\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "class RNN_layer(nn.Module):\n",
    "    def __init__(self,input_size,output_size,num_layers):\n",
    "        \"\"\"_RNN layer 만든거_\n",
    "\n",
    "        Args:\n",
    "            input_size (_int_): _input feature의 개수_\n",
    "            output_size (_int_): _output feature의 개수_\n",
    "            num_layers (_int_): _필요한 RNN layer 수_\n",
    "        \"\"\"\n",
    "        super(RNN_layer, self).__init__()\n",
    "        KAN_model = KAN([input_size,2*input_size+1,input_size*2-1],grid=1)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layer = num_layers\n",
    "        \n",
    "        \n",
    "        ## QNE 수행할 Linear layer\n",
    "        self.QNE_layer = KAN_model \n",
    "        \n",
    "        ## Ansatz parameter\n",
    "        self.ansatz_params_1 = nn.Parameter(torch.rand([8,8]),requires_grad=True)\n",
    "        self.ansatz_params_2 = nn.Parameter(torch.rand([8,8]),requires_grad=True)\n",
    "        self.rnn_layer = RNN_block(input_size)\n",
    "\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            inputs (_torch tensor_): _(batch,seq_len,feature_size)_\n",
    "        \"\"\"\n",
    "        batch = inputs.shape[0]\n",
    "        initial_t = generate_tensor(30,[inputs.shape[0],inputs.shape[2]])\n",
    "        inputs = inputs.permute(1, 0, 2)\n",
    "        ## inputs  = (seq_len,batch,feature_size)\n",
    "        hidden,y_list = self.rnn_layer(inputs[0],initial_t)\n",
    "        for input in inputs[1:]:\n",
    "            hidden,y = self.rnn_layer(input,hidden)\n",
    "\n",
    "            y_list = torch.concat([y_list,y])\n",
    "        y_list = torch.reshape(y_list,[batch,-1])\n",
    "        return y_list\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 10])\n",
      "Output: tensor([[ 0.5289, -0.9233, -1.1201, -0.9498,  0.3260, -1.0280, -0.3502,  0.1797,\n",
      "          0.6081,  0.2400],\n",
      "        [-0.1470, -0.4489, -0.3574, -0.3699,  0.0362,  0.2395, -0.8719, -0.0908,\n",
      "         -0.0704,  0.5883],\n",
      "        [ 0.4971, -0.3949,  0.7621, -0.6622, -0.1307, -1.0103,  0.2518, -0.8066,\n",
      "         -0.0654, -0.3134],\n",
      "        [ 0.5257, -0.2239,  0.1284, -0.5881,  0.4930, -0.1688,  0.3405, -0.7374,\n",
      "         -1.0509,  0.0868],\n",
      "        [ 0.2708, -0.5211, -0.0813, -0.0014, -0.0368,  0.0295,  0.1484, -1.0277,\n",
      "         -0.7533,  0.2138]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "def test_rnn_layer():\n",
    "    # Parameters for the RNN layer\n",
    "    input_size = 8\n",
    "    output_size = 8\n",
    "    num_layers = 2\n",
    "\n",
    "    # Initialize the RNN layer\n",
    "    rnn_layer = RNN_layer(input_size, output_size, num_layers)\n",
    "\n",
    "    # Generate dummy input data\n",
    "    batch_size = 5\n",
    "    seq_len = 10\n",
    "    feature_size = input_size\n",
    "    inputs = torch.randn(batch_size, seq_len, feature_size)\n",
    "\n",
    "    # Forward pass through the RNN layer\n",
    "    output = rnn_layer(inputs)\n",
    "\n",
    "    # Print the output shape and the output itself\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output:\", output)\n",
    "\n",
    "test_rnn_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
